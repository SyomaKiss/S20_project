{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import saver\n",
    "import os\n",
    "from os.path import join, exists\n",
    "\n",
    "import albumentations as albu\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import utils\n",
    "from utils.transforms import get_augmentations, get_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = saver.load_config_dump(\"configs/heatmap_augmentations_weightedloss_borders.yaml\")\n",
    "config.dataset.train['csv_path']= \"playground/synth_nodule.csv\"\n",
    "config.dataset.val['csv_path']= \"both_clean.csv\"\n",
    "config.dataset.test['csv_path']= \"generated_annotation.csv\"\n",
    "\n",
    "config.dataset.root['NIH'] = \"/home/ailab_user/work/data/NIH/images\"\n",
    "config.dataset.root['generated'] = \"\"\n",
    "config.dataset.augmentations = []\n",
    "config.dataset.transforms\n",
    "config.protocol = \"ae\"\n",
    "config.dataset.img_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prune_df(sdf):\n",
    "    df = pd.read_csv('generated_annotation.csv')\n",
    "    old_len = len(sdf)\n",
    "    entries = df['Image Index'].apply(lambda x: x[17:-36]+'.png').values\n",
    "    sdf = sdf[sdf['Image Index'].apply(lambda x: x not in entries)]\n",
    "    print('DF pruned from {} to {} # of entries in order not to contain processed samples'.format(old_len, len(sdf)))\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, config, phase='train'):\n",
    "        self.df = a\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(config)\n",
    "        self.augmentations = get_augmentations(config) if 'train' in phase else None\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) * (self.config.dataset.repeat_dataset+1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "\n",
    "        :param idx:\n",
    "        :return: {..., bbox(4, 2): [upLeft, upRight, downLeft, downRight]}\n",
    "        \"\"\"\n",
    "        idx %= len(self.df)\n",
    "        img_name = self.df.loc[idx, 'Image Index']\n",
    "        img_path = os.path.join(self.config.dataset.root[self.df.loc[idx,'Dataset']],\n",
    "                                img_name)\n",
    "        \n",
    "        img_path = img_path.replace(\".IMG\",\".png\")\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        bbox = np.array(eval(self.df.loc[idx, 'bbox']))\n",
    "\n",
    "        resize_pair = albu.Compose([albu.Resize(self.config.dataset.img_size, self.config.dataset.img_size)],\n",
    "                                   keypoint_params={'format': 'xy', \"remove_invisible\": False})\n",
    "        augmented = resize_pair(image=image, keypoints=bbox)\n",
    "\n",
    "        image = augmented['image']\n",
    "        bbox = np.round(np.array(augmented['keypoints'])).astype(int)\n",
    "\n",
    "        target = np.copy(image)\n",
    "        target[bbox[0, 1]:bbox[2, 1], bbox[0, 0]:bbox[1, 0], :] = 0\n",
    "\n",
    "        sample = {'image': target, 'target': image, 'bbox': bbox, 'img_name': img_name}\n",
    "\n",
    "        if self.augmentations and 'train' in self.phase:\n",
    "            augmented = self.augmentations(image=sample['image'], target=sample['target'], keypoints=sample['bbox'])\n",
    "            if ((np.array(augmented['keypoints']).min() < 0) or\n",
    "                    (np.array(augmented['keypoints']).max() > self.config.dataset.img_size)):\n",
    "                print('Wrong augmentations')\n",
    "            else:\n",
    "                sample['image'] = augmented['image']\n",
    "                sample['target'] = augmented['target']\n",
    "                sample['bbox'] = np.array(augmented['keypoints'])\n",
    "\n",
    "        if self.transforms:\n",
    "            # Apply transform to numpy.ndarray which represents sample image\n",
    "            transformed = self.transforms(image=sample['image'], target=sample['target'], keypoints=sample['bbox'])\n",
    "            sample['image'] = transformed['image']\n",
    "            sample['target'] = transformed['target']\n",
    "            sample['bbox'] = torch.tensor(transformed['keypoints'])\n",
    "\n",
    "        sample['image'] = sample['image'].float()\n",
    "        sample['target'] = sample['target'].float()\n",
    "        sample['bbox'] = sample['bbox'].float()\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF pruned from 9 to 9 # of entries in order not to contain processed samples\n"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv(config.dataset[\"val\"].csv_path)\n",
    "a = a[a.Dataset != \"JSRT\"].reset_index()\n",
    "a = prune_df(a).reset_index()\n",
    "a = a.sample(frac=1).reset_index(drop=True)\n",
    "ds = CustomDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import importlib\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from utils.activations import get_activation\n",
    "from utils.param_initialization import get_init_func\n",
    "models_module = importlib.import_module('protocols.{}.models'.format(config.protocol))\n",
    "model = getattr(models_module, config.model.name)()\n",
    "model = nn.Sequential(model, get_activation(config))\n",
    "\n",
    "path_to_weights = \"/home/ailab_user/work/CancerAstro/playground/G_latest.pth\"\n",
    "state_dict = torch.load(path_to_weights)\n",
    "if \"module\" in list(state_dict.keys())[0]:\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:]  # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    state_dict = new_state_dict\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from skimage.transform import match_histograms\n",
    "from skimage import exposure\n",
    "import cv2\n",
    "def hist_match(img, ref, bbox=None, rounds=1, margin = 5):\n",
    "    img = img.copy()\n",
    "    if bbox is not None and isinstance(bbox, np.ndarray):\n",
    "        bbox = bbox.astype(int)\n",
    "        y_min, y_max = bbox[:, 1].min() - margin, bbox[:, 1].max() + margin\n",
    "        x_min, x_max = bbox[:, 0].min() - margin, bbox[:, 0].max() + margin\n",
    "        for _ in range(rounds):\n",
    "            img[y_min:y_max, x_min:x_max] = match_histograms(img[y_min:y_max, x_min:x_max], ref[y_min:y_max, x_min:x_max])\n",
    "    else:\n",
    "        for _ in range(rounds):\n",
    "            img = match_histograms(img, ref)\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "def sharpen(img, kernel, bbox=None, rounds=1):\n",
    "    img = img.copy()\n",
    "    if bbox is not None and isinstance(bbox, np.ndarray):\n",
    "        bbox = bbox.astype(int)\n",
    "        margin = 0\n",
    "        y_min, y_max = bbox[:, 1].min() - margin, bbox[:, 1].max() + margin\n",
    "        x_min, x_max = bbox[:, 0].min() - margin, bbox[:, 0].max() + margin\n",
    "        for _ in range(rounds):\n",
    "            img[y_min:y_max, x_min:x_max] = cv2.filter2D(img[y_min:y_max, x_min:x_max], -1, kernel)\n",
    "    else:\n",
    "        for _ in range(rounds):\n",
    "            img = cv2.filter2D(img, -1, kernel)\n",
    "    return img\n",
    "\n",
    "def apply(img, func, bbox=None, rounds=1, margin = 5, only_border=False,  **kwargs):\n",
    "    img = img.copy()\n",
    "    if bbox is not None and isinstance(bbox, np.ndarray):\n",
    "        bbox = bbox.astype(int)\n",
    "        y_min, y_max = bbox[:, 1].min() - margin, bbox[:, 1].max() + margin\n",
    "        x_min, x_max = bbox[:, 0].min() - margin, bbox[:, 0].max() + margin\n",
    "        \n",
    "        y_min_center_box, y_max_center_box = bbox[:, 1].min() + margin, bbox[:, 1].max() - margin\n",
    "        x_min_center_box, x_max_center_box = bbox[:, 0].min() + margin, bbox[:, 0].max() - margin\n",
    "        \n",
    "        if only_border:\n",
    "            for _ in range(rounds):\n",
    "                img[y_min:y_min_center_box, x_min:x_max] = func(img[y_min:y_min_center_box, x_min:x_max], **kwargs)\n",
    "                img[y_max_center_box:y_max, x_min:x_max] = func(img[y_max_center_box:y_max, x_min:x_max], **kwargs)\n",
    "                img[y_min_center_box:y_max_center_box, x_min:x_min_center_box] = func(img[y_min_center_box:y_max_center_box, x_min:x_min_center_box], **kwargs)\n",
    "                img[y_min_center_box:y_max_center_box, x_max_center_box:x_max] = func(img[y_min_center_box:y_max_center_box, x_max_center_box:x_max], **kwargs)\n",
    "        else:\n",
    "            for _ in range(rounds):\n",
    "                img[y_min:y_max, x_min:x_max] = func(img[y_min:y_max, x_min:x_max], **kwargs)\n",
    "    else:\n",
    "        for _ in range(rounds):\n",
    "            img = func(img, **kwargs)\n",
    "    return img\n",
    "\n",
    "kernels = {'sharpen': np.array([[0,-1,0],\n",
    "                       [-1,5,-1], \n",
    "                       [0,-1,0]]),\n",
    "          'sharpen_mask': \n",
    "          np.array([[1,4,6,4,1],\n",
    "                   [4,16,24,16,4],\n",
    "                   [6,24,-476, 24,6],\n",
    "                   [4,16,24,16,4],\n",
    "                   [1,4,6,4,1]]) * -1 / 256,\n",
    "          }\n",
    "def post_proc(out, trg, bbox):\n",
    "    \"\"\"\n",
    "    out: img with generated nodule\n",
    "    trg: initial image\n",
    "    bbox: bbox ^)\n",
    "    \"\"\"\n",
    "    ret = out.copy()\n",
    "\n",
    "    ret = sharpen(ret, kernels['sharpen_mask'], bbox, rounds = 1);\n",
    "    ret = apply(ret, cv2.medianBlur, bbox=bbox, rounds=1, ksize=3);\n",
    "    \n",
    "    ret = sharpen(ret, kernels['sharpen_mask'], bbox, rounds = 1);\n",
    "    ret = apply(ret, cv2.medianBlur, bbox=bbox, rounds=1, ksize=5);\n",
    "    \n",
    "    ret = sharpen(ret, kernels['sharpen_mask'], bbox, rounds = 1);\n",
    "    ret = apply(ret, cv2.medianBlur, bbox=bbox, rounds=1, ksize=3);\n",
    "    \n",
    "    ret = hist_match(ret, trg, bbox=bbox)\n",
    "    return ret\n",
    "\n",
    "def post_proc2(out, trg, bbox):\n",
    "    \"\"\"\n",
    "    out: img with generated nodule\n",
    "    trg: initial image\n",
    "    bbox: bbox ^)\n",
    "    \"\"\"\n",
    "    ret = out.copy()\n",
    "\n",
    "    ret = hist_match(ret, trg, bbox=bbox, margin=0)\n",
    "    ret = apply(ret, cv2.medianBlur, bbox=bbox,\n",
    "                rounds=1, margin=2, ksize=5, only_border=False);\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "%matplotlib qt\n",
    "import time\n",
    "\n",
    "i = 1\n",
    "j = 0\n",
    "\n",
    "data = ds[i]\n",
    "img = data['target'].permute(1,2,0)\n",
    "bbox = []\n",
    "pos = []\n",
    "is_bbox_plotted = 0\n",
    "scatter_obj1 = None\n",
    "scatter_obj2 = None\n",
    "\n",
    "def onclick(event):\n",
    "    global pos\n",
    "    global i\n",
    "    global img,bbox\n",
    "    pos.append([event.xdata,event.ydata])\n",
    "    data = ds[i]\n",
    "    img = data['target']\n",
    "    pic.set_data(img.permute(1,2,0))\n",
    "    \n",
    "    if len(pos) == 2:\n",
    "        #[upLeft, upRight, downLeft, downRight]}\n",
    "        pos = np.array(pos)\n",
    "        pos = [pos[0],(pos[1][0],pos[0][1]),(pos[0][0],pos[1][1]),pos[1]]\n",
    "        #radius = 6\n",
    "      #  pos = pos[0]\n",
    "       # pos = [(pos[0]-radius,pos[1]-radius),(pos[0]+radius,pos[1]-radius),(pos[0]-radius,pos[1]+radius),(pos[0]+radius,pos[1]+radius)]\n",
    "        bbox = np.round(np.array(pos)).astype(int)\n",
    "        target = np.copy(img)\n",
    "        target[:,bbox[0, 1]:bbox[2, 1], bbox[0, 0]:bbox[1, 0]] = 0\n",
    "        output = model(torch.tensor(target).unsqueeze(0))[0].permute(1,2,0)\n",
    "        pic.set_data(output.detach().numpy())\n",
    "        f.canvas.draw()\n",
    "        f.canvas.flush_events()\n",
    "        pos = []\n",
    "        \n",
    "def onpress(event):\n",
    "    global i,j\n",
    "    global pic2,bbox\n",
    "    global tag\n",
    "    global is_bbox_plotted, scatter_obj1, scatter_obj2\n",
    "    global annotation\n",
    "    global data\n",
    "    tag = 'init'\n",
    "    if event.key == \"x\":\n",
    "        i+=1\n",
    "        img = get_current_image()\n",
    "        pic.set_data(img)\n",
    "        bbox = []\n",
    "        f.canvas.draw()\n",
    "        f.canvas.flush_events()\n",
    "    if event.key == \"z\":\n",
    "        i-=1\n",
    "        img = get_current_image()\n",
    "        pic.set_data(img)\n",
    "        bbox = []\n",
    "        f.canvas.draw()\n",
    "        f.canvas.flush_events()\n",
    "    if event.key == \"c\":\n",
    "        cur_img = pic.get_array()\n",
    "        orig_img = get_current_image().numpy()\n",
    "        postprocess = process[j](cur_img,orig_img,bbox)\n",
    "        tag = process[j].__name__\n",
    "        if j==len(process)-1:\n",
    "            j = 0\n",
    "        else:\n",
    "            j+=1\n",
    "        pic2 = ax[1].imshow(postprocess)\n",
    "        f.canvas.draw()\n",
    "        f.canvas.flush_events()\n",
    "    if event.key == \"a\":\n",
    "        img = pic2.get_array().filled().transpose(0,1,2)\n",
    "        img_name = generate_name(tag)\n",
    "        annotation = annotation.append({'bbox': str(bbox.tolist()), 'Image Index': img_name,\n",
    "                                       'Dataset': 'generated'}, ignore_index=True)\n",
    "        annotation.to_csv('generated_annotation.csv', index=False)\n",
    "        im = Image.fromarray((img * 255).astype(np.uint8))\n",
    "        im.save(img_name)\n",
    "    if event.key == \"b\":\n",
    "        if data['img_name'].startswith('generated_images'):\n",
    "            bbox = data['bbox']\n",
    "        if is_bbox_plotted:\n",
    "            scatter_obj1.remove()\n",
    "            scatter_obj2.remove()\n",
    "            \n",
    "            f.canvas.draw()\n",
    "            f.canvas.flush_events()\n",
    "\n",
    "            is_bbox_plotted = 0\n",
    "        elif not len(bbox) == 0:\n",
    "            scatter_obj1 = ax[0].scatter(bbox[:, 0], bbox[:, 1], c='r', s=2 ** 3)\n",
    "            scatter_obj2 = ax[1].scatter(bbox[:, 0], bbox[:, 1], c='r', s=2 ** 3)\n",
    "            is_bbox_plotted = 1\n",
    "            f.canvas.draw()\n",
    "            f.canvas.flush_events()\n",
    "\n",
    "        \n",
    "def generate_name(tag):\n",
    "    return 'generated_images/' + ds[i]['img_name'][:-4]+'_' + \\\n",
    "                str(datetime.datetime.now().strftime(\"%d-%m-%Y_(%H-%M-%S)\")) + \"_synthetic.png\"\n",
    "\n",
    "def identity_proc(out, trg, bbox):\n",
    "    return out\n",
    "\n",
    "def get_current_image():\n",
    "    global data\n",
    "    data = ds[i]\n",
    "    img = data['target'].permute(1,2,0)\n",
    "    return img\n",
    "\n",
    "process = [post_proc,post_proc2,identity_proc]\n",
    "f, ax = plt.subplots(1,2, figsize=(60,30), sharey=True)\n",
    "a = ax[0]\n",
    "pic = a.imshow(img)\n",
    "plt.tight_layout()\n",
    "pic2 = []\n",
    "annotation = pd.read_csv('generated_annotation.csv')\n",
    "\n",
    "f.canvas.mpl_connect(\"key_press_event\",onpress)\n",
    "f.canvas.mpl_connect('button_press_event', onclick)\n",
    "f.show()\n",
    "\n",
    "'''\n",
    "You can save only after postproc applied\n",
    "generated_annotation.csv must exist before\n",
    "to see Generated images with bbox's set path to generated_images.csv\n",
    "Show only not processed before images\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Image Index</th>\n",
       "      <th>bbox</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>generated_images/00027224_000_13-02-2020_(15-2...</td>\n",
       "      <td>[[114, 234], [130, 234], [114, 254], [130, 254]]</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>generated_images/00021009_000_13-02-2020_(15-2...</td>\n",
       "      <td>[[114, 211], [133, 211], [114, 233], [133, 233]]</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>generated_images/00000699_000_13-02-2020_(15-2...</td>\n",
       "      <td>[[144, 140], [168, 140], [144, 164], [168, 164]]</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>generated_images/00019961_001_13-02-2020_(15-3...</td>\n",
       "      <td>[[171, 163], [191, 163], [171, 178], [191, 178]]</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NaN</td>\n",
       "      <td>generated_images/00018185_000_13-02-2020_(15-3...</td>\n",
       "      <td>[[357, 226], [374, 226], [357, 245], [374, 245]]</td>\n",
       "      <td>generated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                        Image Index  \\\n",
       "35         NaN  generated_images/00027224_000_13-02-2020_(15-2...   \n",
       "36         NaN  generated_images/00021009_000_13-02-2020_(15-2...   \n",
       "37         NaN  generated_images/00000699_000_13-02-2020_(15-2...   \n",
       "38         NaN  generated_images/00019961_001_13-02-2020_(15-3...   \n",
       "39         NaN  generated_images/00018185_000_13-02-2020_(15-3...   \n",
       "\n",
       "                                                bbox    Dataset  \n",
       "35  [[114, 234], [130, 234], [114, 254], [130, 254]]  generated  \n",
       "36  [[114, 211], [133, 211], [114, 233], [133, 233]]  generated  \n",
       "37  [[144, 140], [168, 140], [144, 164], [168, 164]]  generated  \n",
       "38  [[171, 163], [191, 163], [171, 178], [191, 178]]  generated  \n",
       "39  [[357, 226], [374, 226], [357, 245], [374, 245]]  generated  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('generated_annotation.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
